---
title: "2024 Monitoring Plan"
author: "Emma Jones"
date: "9/28/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

# built in R 3.6.2
library(tidyverse)
library(sf)
library(config)
library(DBI)
library(pool)
library(dbplyr)
library(leaflet)
library(inlmisc)
library(DT)
library(lubridate)
library(readxl)
library(pins)

# Get configuration settings
conn <- config::get("connectionSettings")

board_register_rsconnect(key = conn$CONNECT_API_KEY,  #Sys.getenv("CONNECT_API_KEY"),
                         server = conn$CONNECT_SERVER)#Sys.getenv("CONNECT_SERVER"))

## Connect to ODS production
pool <- dbPool(
  drv = odbc::odbc(),
  Driver = "ODBC Driver 11 for SQL Server",#Driver = "SQL Server Native Client 11.0",
  Server= "DEQ-SQLODS-PROD,50000",
  dbname = "ODS",
  trusted_connection = "yes"
)
```

## Project Background

BRRO's monitoring planning process is being updated to a more reproducible and efficient collaborative process that utilizes modern project sharing techniques. This report serves as one project archive to aid in future monitoring plan development.

The general steps of a monitoring plan development involve identifying where previous plans have sampled and what frequency, comparing that information with longer term sampling goals, and identifying stations to be monitored in the upcoming year. After the plan is mapped out for the forthcoming season, new stations are created (as needed) in CEDS, monitoring staff build runs to efficiently collect samples and program these runs in CEDS. After a region is done with their monitoring plan, Central Office staff review the plan to ensure statewide goals and budget are met, providing feedback for the region if needed.

### Data Gathering

First, we will bring in the 2024 IR dataset to get a longer term view of sampling efforts in BRRO. This dataset encompasses 2017-2022. The station project codes and sample frequency are useful to the planning process to ensure sampling goals have been met. 

After this data is summarized, we will pull the 2023 sample information (to date) from CEDS to run through a similar process. 

By mapping this information we can quickly identify areas that might be lacking data for upcoming assessment windows and correct for potential data gaps.


```{r bring in precompiled spatial data}
WQM_Stations_Spatial <- pin_get("ejones/WQM-Stations-Spatial", board = "rsconnect") %>%
  rename("Basin_Name" = "Basin_Code")   # can't have same name different case when using sqldf
```

```{r conventionals IR2024}

conventionals <-  pin_get('ejones/conventionals2024final', board = 'rsconnect') %>% 
   # get it in real conventionals format
  rename('Deq_Region'='VADEQ_ADMIN_REGION', 'STA_REC_CODE' = 'MONITORING_REGION') %>% #,
         # 'ECOLI' = 'E.COLI_ECOLI_CFU/100mL', 'ENTEROCOCCI' = 'ENTEROCOCCI_31649_NO/100mL')  %>%
  left_join(WQM_Stations_Spatial, by = c("FDT_STA_ID" = 'StationID'))

```

```{r conventionals IR2024}

# conventionals <-  pin_get('ejones/conventionals2024final', board = 'rsconnect') %>% 
#    # get it in real conventionals format
#   rename('Deq_Region'='VADEQ_ADMIN_REGION', 'STA_REC_CODE' = 'MONITORING_REGION') %>% #,
#          # 'ECOLI' = 'E.COLI_ECOLI_CFU/100mL', 'ENTEROCOCCI' = 'ENTEROCOCCI_31649_NO/100mL')  %>%
#   left_join(WQM_Stations_Spatial, by = c("FDT_STA_ID" = 'StationID'))
# 

# # Takes a while so only do this once and save BRRO subset locally
# conventionals <- read_excel('C:/HardDriveBackup/R/GitHub/IR2022/2.organizeMetadata/data/final2022data/CEDSWQM/CONVENTIONALS_20210504.xlsx') %>%
#   # get it in real conventionals format
#   rename('Deq_Region'='VADEQ_ADMIN_REGION', 'STA_REC_CODE' = 'MONITORING_REGION',
#          'ECOLI' = 'E.COLI_ECOLI_CFU/100mL', 'ENTEROCOCCI' = 'ENTEROCOCCI_31649_NO/100mL')  %>%
#   left_join(WQM_Stations_Spatial, by = c("FDT_STA_ID" = 'StationID')) 
# 
# # repeat for each region
# conventionalsRegion <- conventionals %>%
#   filter(ASSESS_REG == region)
# saveRDS(conventionalsRegion, paste0('data/IR2024conventionals', region, '.RDS'))
# # rm(conventionalsRegion)
# conventionals <- readRDS(paste0('../for2022/data/IR2022conventionals', region, '.RDS')  )

```


Now pull sample info for 2023-Sys.Date sample data from CEDS. Certain Survey Program codes are dropped bc they don't directly relate to WQM activities. These are excluded from any field data pulled from ODS. More codes could be included in this list with more input from regions.


Run this for each region

```{r 2023 sample data from CEDS}
region <- "BRRO"

WQM_Stations_Spatial_Region <-  filter(WQM_Stations_Spatial, ASSESS_REG == region)


sampleWindow <- c(as.Date('2023-01-01'), as.Date(Sys.Date()))
windowInfo <- pool %>% tbl( in_schema("wqm", "Wqm_Field_Data_View")) %>%
  filter(Fdt_Sta_Id %in% !! WQM_Stations_Spatial_Region$StationID &
           between(as.Date(Fdt_Date_Time), !! sampleWindow[1], !! sampleWindow[2])) %>%
  filter(! Fdt_Spg_Code %in% c("GW", "FM", "HW", "NT", "PO", "IR", "AM", "FI", "QA", "PC")) %>%  #drop codes not used for WQM
  as_tibble() 

```


Now we can combine these datasets and summarize sample information

```{r combine data and summarize}

windowSampleInfo <- conventionals %>% 
    filter(ASSESS_REG == region) %>% 
  dplyr::select(FDT_STA_ID, FDT_SPG_CODE, FDT_DATE_TIME ) %>% 
  bind_rows(
    dplyr::select(windowInfo, FDT_STA_ID = Fdt_Sta_Id, FDT_SPG_CODE = Fdt_Spg_Code, FDT_DATE_TIME = Fdt_Date_Time)) %>% 
  mutate(Year = year(FDT_DATE_TIME)) %>% #paste0(year(FDT_DATE_TIME)), ' Sample n') %>% 
  group_by(FDT_STA_ID, Year) %>% 
  summarise(`Sample n` = n(),
            `SPG codes` = paste(unique(FDT_SPG_CODE),  collapse = ' | '),
            `SPG codes First` = as.factor(unique(FDT_SPG_CODE)[1])) %>% # keep this first one for mapping purposes
  mutate(IR2024 = case_when(Year %in% c(2017:2022) ~ 'IR 2024',
                                       TRUE ~ as.character(NA)),
         IR2026 = case_when(Year %in% c(2019:2024) ~ 'IR 2026',
                                       TRUE ~ as.character(NA)),
         IR2028 = case_when(Year %in% c(2021:2026) ~ 'IR 2028',
                                       TRUE ~ as.character(NA)),
         IR2030 = case_when(Year %in% c(2023:2028) ~ 'IR 2030',
                                       TRUE ~ as.character(NA))) %>% 
  group_by(FDT_STA_ID, IR2024 ) %>% 
  mutate(`IR2024 Sample n` = case_when(IR2024 == 'IR 2024' ~ sum(`Sample n`))) %>% 
  ungroup() %>%  group_by(FDT_STA_ID, IR2026 ) %>% 
  mutate(`IR2026 Sample n` = case_when(IR2026 == 'IR 2026' ~ sum(`Sample n`))) %>% 
  ungroup() %>%  group_by(FDT_STA_ID, IR2028 ) %>% 
   mutate(`IR2028 Sample n` = case_when(IR2028 == 'IR 2028' ~ sum(`Sample n`))) %>% 
  ungroup() %>%
  group_by(FDT_STA_ID, IR2030 ) %>% 
  mutate(`IR2030 Sample n` = case_when(IR2030 == 'IR 2030' ~ sum(`Sample n`))) %>% 
   ungroup() %>%
  dplyr::select(-c(IR2024, IR2026, IR2028, IR2030)) 

windowSampleInfoSummary <- dplyr::select(windowSampleInfo, -c(`IR2024 Sample n`, `IR2026 Sample n`,
                                                               `IR2028 Sample n`, `IR2030 Sample n`)) %>% 
  group_by(FDT_STA_ID) %>% 
  pivot_wider(names_from = 'Year', names_prefix = "Year", values_from = c('Sample n', 'SPG codes', 'SPG codes First')) 


#old skool rename of columns
names(windowSampleInfoSummary)[2:length(names(windowSampleInfoSummary))] <-
  paste(strsplit(names(windowSampleInfoSummary)[2:length(names(windowSampleInfoSummary))], '_') %>% 
         map_chr(2),
       strsplit(names(windowSampleInfoSummary)[2:length(names(windowSampleInfoSummary))], '_') %>% 
         map_chr(1), sep = ' ')

# rearrange column order to make sense
windowSampleInfoFinalSummary <- windowSampleInfoSummary %>% 
  select(sort(tidyselect::peek_vars())) %>% 
  left_join(dplyr::select(windowSampleInfo, FDT_STA_ID, `IR2024 Sample n`, 
                          `IR2026 Sample n`, `IR2028 Sample n`, `IR2030 Sample n` ) %>% 
              pivot_longer(cols = c(`IR2024 Sample n`, 
                                    `IR2026 Sample n`, `IR2028 Sample n`, `IR2030 Sample n`), names_to = 'window', values_to = 'value') %>% 
              group_by(FDT_STA_ID, window) %>%
              filter(value > 0) %>% ungroup() %>% 
              group_by(FDT_STA_ID, window) %>% 
              mutate(n = 1:n()) %>% 
              filter(n == 1) %>% dplyr::select(-n) %>% 
              pivot_wider(names_from = 'window', values_from = 'value'), by = 'FDT_STA_ID') %>% 
  left_join(dplyr::select(WQM_Stations_Spatial, StationID:Sta_Desc, ASSESS_REG, VAHU6), by = c('FDT_STA_ID' = 'StationID')) %>% 
  dplyr::select( FDT_STA_ID, Sta_Desc, ASSESS_REG, VAHU6, `IR2024 Sample n`, 
                 `IR2026 Sample n`,  `IR2028 Sample n`, `IR2030 Sample n`,  everything()) %>% 
  st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
           remove = T, # don't remove these lat/lon cols from df
           crs = 4326)
```

Save as list object for use in app.

```{r}
# necessary spatial info
assessmentRegionsVAHU6 <- st_as_sf(pin_get('ejones/vahu6', board = 'rsconnect')) %>% 
  filter(ASSESS_REG == region) %>%
  dplyr::select(VAHU6, ASSESS_REG)

regionInfo <- list(
  sampleWindow = sampleWindow,
  windowInfo = windowInfo,
  conventionals = conventionalsRegion,
  windowSampleInfoFinalSummary = windowSampleInfoFinalSummary %>%  
    left_join(dplyr::select(WQM_Stations_Spatial_Region, StationID, Latitude, Longitude),
              by = c('FDT_STA_ID' = 'StationID')) %>% 
    st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
                                  remove = T, # don't remove these lat/lon cols from df
                                  crs = 4326),
  x2017 = filter( windowSampleInfoFinalSummary, !is.na(`Year2017 Sample n`)) %>% 
    left_join(dplyr::select(WQM_Stations_Spatial_Region, StationID, Latitude, Longitude),
              by = c('FDT_STA_ID' = 'StationID')) %>% 
    mutate(UID = paste(FDT_STA_ID, 2017, sep = '/')) %>% # UID for mapping
    st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
                 remove = T, # don't remove these lat/lon cols from df
                 crs = 4326) ,
  x2018 = filter( windowSampleInfoFinalSummary, !is.na(`Year2018 Sample n`)) %>% 
    left_join(dplyr::select(WQM_Stations_Spatial_Region, StationID, Latitude, Longitude),
              by = c('FDT_STA_ID' = 'StationID')) %>% 
    mutate(UID = paste(FDT_STA_ID, 2018, sep = '/')) %>% # UID for mapping
    st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
                 remove = T, # don't remove these lat/lon cols from df
                 crs = 4326) ,
  x2019 = filter( windowSampleInfoFinalSummary, !is.na(`Year2019 Sample n`)) %>% 
    left_join(dplyr::select(WQM_Stations_Spatial_Region, StationID, Latitude, Longitude),
              by = c('FDT_STA_ID' = 'StationID')) %>% 
    mutate(UID = paste(FDT_STA_ID, 2019, sep = '/')) %>% # UID for mapping
    st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
                 remove = T, # don't remove these lat/lon cols from df
                 crs = 4326) ,
  x2020 = filter( windowSampleInfoFinalSummary, !is.na(`Year2020 Sample n`))%>% 
    left_join(dplyr::select(WQM_Stations_Spatial_Region, StationID, Latitude, Longitude),
              by = c('FDT_STA_ID' = 'StationID')) %>% 
    mutate(UID = paste(FDT_STA_ID, 2020, sep = '/')) %>% # UID for mapping
    st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
                 remove = T, # don't remove these lat/lon cols from df
                 crs = 4326) ,
  x2021 = filter( windowSampleInfoFinalSummary, !is.na(`Year2021 Sample n`)) %>% 
    left_join(dplyr::select(WQM_Stations_Spatial_Region, StationID, Latitude, Longitude),
              by = c('FDT_STA_ID' = 'StationID')) %>% 
    mutate(UID = paste(FDT_STA_ID, 2021, sep = '/')) %>% # UID for mapping
    st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
                 remove = T, # don't remove these lat/lon cols from df
                 crs = 4326) ,
  x2022 = filter( windowSampleInfoFinalSummary, !is.na(`Year2022 Sample n`)) %>% 
    left_join(dplyr::select(WQM_Stations_Spatial_Region, StationID, Latitude, Longitude),
              by = c('FDT_STA_ID' = 'StationID')) %>% 
    mutate(UID = paste(FDT_STA_ID, 2022, sep = '/')) %>% # UID for mapping
    st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
                 remove = T, # don't remove these lat/lon cols from df
                 crs = 4326) ,
  x2023 = filter( windowSampleInfoFinalSummary, !is.na(`Year2023 Sample n`)) %>% 
    left_join(dplyr::select(WQM_Stations_Spatial_Region, StationID, Latitude, Longitude),
              by = c('FDT_STA_ID' = 'StationID')) %>% 
    mutate(UID = paste(FDT_STA_ID, 2023, sep = '/')) %>% # UID for mapping
    st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
                 remove = T, # don't remove these lat/lon cols from df
                 crs = 4326) ,

  ir2024 = filter( windowSampleInfoFinalSummary, !is.na(`IR2024 Sample n`)) %>% 
  group_by(VAHU6) %>% 
  summarise(`n Stations` = length(unique(FDT_STA_ID)),
          `n Samples` = sum(`IR2024 Sample n`)) %>% 
  # join spatial info next to make sure the tagging gets done properly in next step
  full_join(assessmentRegionsVAHU6) %>% 
  mutate(`IR Sampling Status` = case_when(`n Samples` >= 10 ~ 'Fine',
                                          `n Samples` < 10 ~ '< 10 samples',
                                          is.na(`n Stations`) | `n Samples` == 0 ~ "Need Station",
                                          TRUE ~ as.character(NA))) %>% 
    droplevels(),
  ir2026 = filter( windowSampleInfoFinalSummary, !is.na(`IR2026 Sample n`)) %>% 
  group_by(VAHU6) %>% 
  summarise(`n Stations` = length(unique(FDT_STA_ID)),
          `n Samples` = sum(`IR2026 Sample n`)) %>% 
  # join spatial info next to make sure the tagging gets done properly in next step
  full_join(assessmentRegionsVAHU6) %>% 
  mutate(`IR Sampling Status` = case_when(`n Samples` >= 10 ~ 'Fine',
                                          `n Samples` < 10 ~ '< 10 samples',
                                          is.na(`n Stations`) | `n Samples` == 0 ~ "Need Station",
                                          TRUE ~ as.character(NA))) %>% 
    droplevels(),
  ir2028 = filter( windowSampleInfoFinalSummary, !is.na(`IR2028 Sample n`)) %>% 
  group_by(VAHU6) %>% 
  summarise(`n Stations` = length(unique(FDT_STA_ID)),
          `n Samples` = sum(`IR2028 Sample n`)) %>% 
  # join spatial info next to make sure the tagging gets done properly in next step
  full_join(assessmentRegionsVAHU6) %>% 
  mutate(`IR Sampling Status` = case_when(`n Samples` >= 10 ~ 'Fine',
                                          `n Samples` < 10 ~ '< 10 samples',
                                          is.na(`n Stations`) | `n Samples` == 0 ~ "Need Station",
                                          TRUE ~ as.character(NA))) %>% 
    droplevels(),
  ir2030 = filter( windowSampleInfoFinalSummary, !is.na(`IR2030 Sample n`)) %>% 
  group_by(VAHU6) %>% 
  summarise(`n Stations` = length(unique(FDT_STA_ID)),
          `n Samples` = sum(`IR2030 Sample n`)) %>% 
  # join spatial info next to make sure the tagging gets done properly in next step
  full_join(assessmentRegionsVAHU6) %>% 
  mutate(`IR Sampling Status` = case_when(`n Samples` >= 10 ~ 'Fine',
                                          `n Samples` < 10 ~ '< 10 samples',
                                          is.na(`n Stations`) | `n Samples` == 0 ~ "Need Station",
                                          TRUE ~ as.character(NA))) %>% 
    droplevels()
)

saveRDS(regionInfo, paste0('data/', region,'regionInfo.RDS'))
```




